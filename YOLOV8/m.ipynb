{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 192.1/228.3 GB disk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 192.1/228.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 192.1/228.3 GB disk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 192.1/228.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "class YOLOv8_ObjectDetector:\n",
    "    \"\"\"\n",
    "    A class for performing object detection on images and videos using YOLOv8.\n",
    "\n",
    "    Args:\n",
    "    ------------\n",
    "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
    "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
    "        classes (list[str], optional): Alias for labels. Deprecated.\n",
    "        conf (float, optional): Minimum confidence threshold for object detection.\n",
    "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
    "\n",
    "    Attributes:\n",
    "    --------------\n",
    "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
    "        conf (float): Minimum confidence threshold for object detection.\n",
    "        iou (float): Minimum IOU threshold for non-max suppression.\n",
    "        model (YOLO): The YOLOv8 model used for object detection.\n",
    "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
    "\n",
    "    Methods :\n",
    "    -------------\n",
    "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
    "        custom_display: Returns a custom display of the object detection results.\n",
    "        predict_video: Predicts objects in a video and saves the results to a file.\n",
    "        predict_img: Predicts objects in an image and returns the detection results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
    "\n",
    "        self.classes = classes\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "        self.model = YOLO(model_file)\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "        if labels == None:\n",
    "            self.labels = self.model.names\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs object detection on a single image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            img (numpy.ndarray): Input image to perform object detection on.\n",
    "            verbose (bool): Whether to print detection details.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains \n",
    "             details about detection results :\n",
    "                    - Class IDs\n",
    "                    - Bounding Boxes\n",
    "                    - Confidence score\n",
    "                    ...\n",
    "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the model on the input image with the given parameters\n",
    "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "\n",
    "        # Save the original image and the results for further analysis if needed\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "\n",
    "        # Return the detection results\n",
    "        return results[0]\n",
    "\n",
    "\n",
    "\n",
    "    def default_display(self, show_conf=True, line_width=None, font_size=None, \n",
    "                        font='Arial.ttf', pil=False, example='abc'):\n",
    "        \"\"\"\n",
    "        Displays the detected objects on the original input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show_conf : bool, optional\n",
    "            Whether to show the confidence score of each detected object, by default True.\n",
    "        line_width : int, optional\n",
    "            The thickness of the bounding box line in pixels, by default None.\n",
    "        font_size : int, optional\n",
    "            The font size of the text label for each detected object, by default None.\n",
    "        font : str, optional\n",
    "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
    "        pil : bool, optional\n",
    "            Whether to return a PIL Image object, by default False.\n",
    "        example : str, optional\n",
    "            A string to display on the example bounding box, by default 'abc'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or PIL Image\n",
    "            The original input image with the detected objects displayed as bounding boxes.\n",
    "            If `pil=True`, a PIL Image object is returned instead.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the input image has not been detected by calling the `predict_img()` method first.\n",
    "        \"\"\"\n",
    "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
    "        if self.results is None:\n",
    "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
    "        \n",
    "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
    "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
    "        \n",
    "        # Return the displayed image\n",
    "        return display_img\n",
    "\n",
    "        \n",
    "\n",
    "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
    "        \"\"\"\n",
    "        Custom display method that draws bounding boxes and labels on the original image, \n",
    "        with additional options for showing class and confidence information.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        colors : list\n",
    "            A list of tuples specifying the color of each class.\n",
    "        show_cls : bool, optional\n",
    "            Whether to show class information in the label text. Default is True.\n",
    "        show_conf : bool, optional\n",
    "            Whether to show confidence information in the label text. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            The image with bounding boxes and labels drawn on it.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.orig_img\n",
    "        # calculate the bounding box thickness based on the image width and height\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "\n",
    "            # Extract object class and confidence score\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "\n",
    "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            # Print detection info\n",
    "            if show_cls:\n",
    "                textString += f\"{self.labels[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            # Calculate font scale based on object size\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            # Draw bounding box, a centroid and label on the image\n",
    "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
    "\n",
    "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
    "            \n",
    "             # If there are no details to show on the image\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                # show the details text in a filled rectangle\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString , \n",
    "                    (x1, y1), font, \n",
    "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
    "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            video_path (str): The path to the input video file.\n",
    "            save_dir (str): The path to the directory where the output video file will be saved.\n",
    "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
    "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
    "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
    "            **display_args: Additional arguments to be passed to the display function.\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Open the input video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get the name of the input video file\n",
    "        vid_name = os.path.basename(video_path)\n",
    "\n",
    "        # Get the dimensions of each frame in the input video file\n",
    "        width = int(cap.get(3))  # get `width`\n",
    "        height = int(cap.get(4))  # get `height`\n",
    "\n",
    "        # Create the directory for the output video file if it does not already exist\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Set the name and path for the output video file\n",
    "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
    "        save_file = os.path.join(save_dir, save_name)\n",
    "\n",
    "        # Print information about the input and output video files if verbose is True\n",
    "        if verbose:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
    "            print(f\"RESOLUTION : {width}x{height}\")\n",
    "            print('SAVING TO :' + save_file)\n",
    "\n",
    "        # Define an output VideoWriter object\n",
    "        out = cv2.VideoWriter(save_file,\n",
    "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "                              30, (width, height))\n",
    "\n",
    "        # Check if the input video file was opened correctly\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video stream or file\")\n",
    "\n",
    "        # Read each frame of the input video file\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame was not read successfully, break the loop\n",
    "            if not ret:\n",
    "                print(\"Error reading frame\")\n",
    "                break\n",
    "\n",
    "            # Run object detection on the frame and calculate FPS\n",
    "            beg = time.time()\n",
    "            results = self.predict_img(frame, verbose=False)\n",
    "            if results is None:\n",
    "                print('***********************************************')\n",
    "            fps = 1 / (time.time() - beg)\n",
    "\n",
    "            # Display the detection results\n",
    "            if display == 'default':\n",
    "                frame = self.default_display(**display_args)\n",
    "            elif display == 'custom':\n",
    "                frame == self.custom_display(**display_args)\n",
    "\n",
    "            # Display the FPS on the frame\n",
    "            frame = cv2.putText(frame, f\"FPS : {fps:,.2f}\",\n",
    "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Write the frame to the output video file\n",
    "            out.write(frame)\n",
    "\n",
    "            # Exit the loop if the 'q' button is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # After the loop release the cap and video writer\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        yolo_names = [ 'yolov8m.pt']#, 'yolov8l.pt', 'yolov8x.pt']\n",
    "colors = []\n",
    "for _ in range(80):\n",
    "    rand_tuple = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
    "    colors.append(rand_tuple)\n",
    "\n",
    "detectors = []\n",
    "for yolo_name in yolo_names:\n",
    "    detector = YOLOv8_ObjectDetector(yolo_name, conf = 0.55 )\n",
    "    detectors.append(detector)\n",
    "    for detector in detectors :\n",
    "    print(f\"Model : {detector.model_name}\")\n",
    "    for img_name in os.listdir(test_imgs_path):\n",
    "        print(f\"Target image : {img_name}\")\n",
    "        print(img_name.split('.')[0])\n",
    "        print(\"-------------------------\")\n",
    "        \n",
    "        img = cv2.imread(os.path.join(test_imgs_path, img_name))\n",
    "        results = detector.predict_img(img, verbose = False)\n",
    "        result_img = detector.custom_display(colors = colors)\n",
    "\n",
    "        save_dir = os.path.join(img_results_path, detector.model_name)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_name = detector.model_name + ' -- ' + img_name.split('.')[0] + '.jpg'\n",
    "        save_file = os.path.join(img_results_path, save_dir, save_name) \n",
    "        print(f\"Saving results to : {save_file}\")\n",
    "        cv2.imwrite(save_file, result_img)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "d = YOLOv8_ObjectDetector()\n",
    "print(d.labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_results_path = 'uavdt_detect'\n",
    "\n",
    "test_imgs_path = 'uavdt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(img_results_path):\n",
    "    os.makedirs(img_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_names = [ 'yolov8m.pt']#, 'yolov8l.pt', 'yolov8x.pt']\n",
    "colors = []\n",
    "for _ in range(80):\n",
    "    rand_tuple = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
    "    colors.append(rand_tuple)\n",
    "\n",
    "detectors = []\n",
    "for yolo_name in yolo_names:\n",
    "    detector = YOLOv8_ObjectDetector(yolo_name, conf = 0.55 )\n",
    "    detectors.append(detector)\n",
    "    for detector in detectors :\n",
    "    print(f\"Model : {detector.model_name}\")\n",
    "    for img_name in os.listdir(test_imgs_path):\n",
    "        print(f\"Target image : {img_name}\")\n",
    "        print(img_name.split('.')[0])\n",
    "        print(\"-------------------------\")\n",
    "        \n",
    "        img = cv2.imread(os.path.join(test_imgs_path, img_name))\n",
    "        results = detector.predict_img(img, verbose = False)\n",
    "        result_img = detector.custom_display(colors = colors)\n",
    "\n",
    "        save_dir = os.path.join(img_results_path, detector.model_name)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_name = detector.model_name + ' -- ' + img_name.split('.')[0] + '.jpg'\n",
    "        save_file = os.path.join(img_results_path, save_dir, save_name) \n",
    "        print(f\"Saving results to : {save_file}\")\n",
    "        cv2.imwrite(save_file, result_img)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : yolov8m\n",
      "Target image : img000001.jpg\n",
      "img000001\n",
      "-------------------------\n",
      "Target image : img000002.jpg\n",
      "img000002\n",
      "-------------------------\n",
      "Target image : img000177.jpg\n",
      "img000177\n",
      "-------------------------\n",
      "Target image : img000004.jpg\n",
      "img000004\n",
      "-------------------------\n",
      "Target image : img000207.jpg\n",
      "img000207\n",
      "-------------------------\n",
      "Target image : img000005.jpg\n",
      "img000005\n",
      "-------------------------\n",
      "Target image : img000179.jpg\n",
      "img000179\n",
      "-------------------------\n",
      "Target image : img000178.jpg\n",
      "img000178\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for detector in detectors :\n",
    "    print(f\"Model : {detector.model_name}\")\n",
    "    for img_name in os.listdir(test_imgs_path):\n",
    "        print(f\"Target image : {img_name}\")\n",
    "        print(img_name.split('.')[0])\n",
    "        print(\"-------------------------\")\n",
    "        \n",
    "        img = cv2.imread(os.path.join(test_imgs_path, img_name))\n",
    "        results = detector.predict_img(img, verbose = False)\n",
    "        result_img = detector.custom_display(colors = colors)\n",
    "\n",
    "        save_dir = os.path.join(img_results_path, detector.model_name)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_name = detector.model_name + ' -- ' + img_name.split('.')[0] + '.jpg'\n",
    "        save_file = os.path.join(img_results_path, save_dir, save_name) \n",
    "        print(f\"Saving results to : {save_file}\")\n",
    "        cv2.imwrite(save_file, result_img)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 192.1/228.3 GB disk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 192.1/228.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "class YOLOv8_ObjectDetector:\n",
    "    \"\"\"\n",
    "    A class for performing object detection on images and videos using YOLOv8.\n",
    "\n",
    "    Args:\n",
    "    ------------\n",
    "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
    "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
    "        classes (list[str], optional): Alias for labels. Deprecated.\n",
    "        conf (float, optional): Minimum confidence threshold for object detection.\n",
    "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
    "\n",
    "    Attributes:\n",
    "    --------------\n",
    "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
    "        conf (float): Minimum confidence threshold for object detection.\n",
    "        iou (float): Minimum IOU threshold for non-max suppression.\n",
    "        model (YOLO): The YOLOv8 model used for object detection.\n",
    "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
    "\n",
    "    Methods :\n",
    "    -------------\n",
    "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
    "        custom_display: Returns a custom display of the object detection results.\n",
    "        predict_video: Predicts objects in a video and saves the results to a file.\n",
    "        predict_img: Predicts objects in an image and returns the detection results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
    "\n",
    "        self.classes = classes\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "        self.model = YOLO(model_file)\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "        if labels == None:\n",
    "            self.labels = self.model.names\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs object detection on a single image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            img (numpy.ndarray): Input image to perform object detection on.\n",
    "            verbose (bool): Whether to print detection details.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains \n",
    "             details about detection results :\n",
    "                    - Class IDs\n",
    "                    - Bounding Boxes\n",
    "                    - Confidence score\n",
    "                    ...\n",
    "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the model on the input image with the given parameters\n",
    "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "\n",
    "        # Save the original image and the results for further analysis if needed\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "\n",
    "        # Return the detection results\n",
    "        return results[0]\n",
    "\n",
    "\n",
    "\n",
    "    def default_display(self, show_conf=True, line_width=None, font_size=None, \n",
    "                        font='Arial.ttf', pil=False, example='abc'):\n",
    "        \"\"\"\n",
    "        Displays the detected objects on the original input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show_conf : bool, optional\n",
    "            Whether to show the confidence score of each detected object, by default True.\n",
    "        line_width : int, optional\n",
    "            The thickness of the bounding box line in pixels, by default None.\n",
    "        font_size : int, optional\n",
    "            The font size of the text label for each detected object, by default None.\n",
    "        font : str, optional\n",
    "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
    "        pil : bool, optional\n",
    "            Whether to return a PIL Image object, by default False.\n",
    "        example : str, optional\n",
    "            A string to display on the example bounding box, by default 'abc'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or PIL Image\n",
    "            The original input image with the detected objects displayed as bounding boxes.\n",
    "            If `pil=True`, a PIL Image object is returned instead.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the input image has not been detected by calling the `predict_img()` method first.\n",
    "        \"\"\"\n",
    "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
    "        if self.results is None:\n",
    "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
    "        \n",
    "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
    "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
    "        \n",
    "        # Return the displayed image\n",
    "        return display_img\n",
    "\n",
    "        \n",
    "\n",
    "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
    "        \"\"\"\n",
    "        Custom display method that draws bounding boxes and labels on the original image, \n",
    "        with additional options for showing class and confidence information.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        colors : list\n",
    "            A list of tuples specifying the color of each class.\n",
    "        show_cls : bool, optional\n",
    "            Whether to show class information in the label text. Default is True.\n",
    "        show_conf : bool, optional\n",
    "            Whether to show confidence information in the label text. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            The image with bounding boxes and labels drawn on it.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.orig_img\n",
    "        # calculate the bounding box thickness based on the image width and height\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "\n",
    "            # Extract object class and confidence score\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "\n",
    "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            # Print detection info\n",
    "            if show_cls:\n",
    "                textString += f\"{self.labels[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            # Calculate font scale based on object size\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            # Draw bounding box, a centroid and label on the image\n",
    "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
    "\n",
    "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
    "            \n",
    "             # If there are no details to show on the image\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                # show the details text in a filled rectangle\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString , \n",
    "                    (x1, y1), font, \n",
    "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
    "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            video_path (str): The path to the input video file.\n",
    "            save_dir (str): The path to the directory where the output video file will be saved.\n",
    "            save_format (str, optional): The format for the output video file, by default \"avi\".\n",
    "            display (str, optional): The display mode for the output video, by default 'custom'. \n",
    "            verbose (bool, optional): Whether to print detection details, by default True.\n",
    "\n",
    "        Other Parameters:\n",
    "        -----------\n",
    "            **display_args: Additional keyword arguments to be passed to the display method (default/custom).\n",
    "        \n",
    "        \"\"\"\n",
    "        # Open the input video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        # Set the codec and create a VideoWriter object to save the output video\n",
    "        save_path = os.path.join(save_dir, f\"{self.model_name}.{save_format}\")\n",
    "        out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'MJPG'), fps, (width, height))\n",
    "        \n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        for frame_idx in range(frame_count):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Run object detection on the current frame\n",
    "            results = self.predict_img(frame, verbose)\n",
    "            # Display the detected objects on the frame\n",
    "            if display == 'default':\n",
    "                out_frame = self.default_display(**display_args)\n",
    "            else:\n",
    "                out_frame = self.custom_display(**display_args)\n",
    "            \n",
    "            # Write the output frame to the video file\n",
    "            out.write(out_frame)\n",
    "        \n",
    "        # Release the video capture and writer objects\n",
    "        cap.release()\n",
    "        out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clearml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.16.2)\n",
      "Requirement already satisfied: attrs>=18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (23.2.0)\n",
      "Requirement already satisfied: furl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.1.3)\n",
      "Requirement already satisfied: jsonschema>=2.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from clearml) (1.24.2)\n",
      "Requirement already satisfied: pathlib2>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.3.7.post1)\n",
      "Requirement already satisfied: Pillow>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (9.5.0)\n",
      "Requirement already satisfied: psutil>=3.4.2 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from clearml) (5.9.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from clearml) (2.8.2)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.32.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from clearml) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (1.26.19)\n",
      "Requirement already satisfied: pyjwt<2.9.0,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.8.0)\n",
      "Requirement already satisfied: referencing<0.40 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (0.35.1)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from furl>=2.0.0->clearml) (1.0.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema>=2.6.0->clearml) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema>=2.6.0->clearml) (68.0.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from referencing<0.40->clearml) (0.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20.0->clearml) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20.0->clearml) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20.0->clearml) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install clearml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 ðŸš€ Python-3.11.1 torch-2.0.1 CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 193.1/228.3 GB disk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 193.1/228.3 GB disk)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics.yolo.utils.callbacks.clearml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 102\u001b[0m\n\u001b[1;32m     97\u001b[0m colors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m),  \u001b[38;5;66;03m# red for person\u001b[39;00m\n\u001b[1;32m     98\u001b[0m           \u001b[38;5;241m1\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m),  \u001b[38;5;66;03m# green for car\u001b[39;00m\n\u001b[1;32m     99\u001b[0m           \u001b[38;5;241m2\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)}  \u001b[38;5;66;03m# blue for truck\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Call the predict_and_save_images method to detect objects in all images in the input directory\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_save_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject detection completed. Results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[101], line 83\u001b[0m, in \u001b[0;36mYOLOv8_ObjectDetector.predict_and_save_images\u001b[0;34m(self, image_dir, save_dir, display, **display_args)\u001b[0m\n\u001b[1;32m     81\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, filename)\n\u001b[1;32m     82\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path)\n\u001b[0;32m---> 83\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     85\u001b[0m     out_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_display(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdisplay_args)\n",
      "Cell \u001b[0;32mIn[101], line 27\u001b[0m, in \u001b[0;36mYOLOv8_ObjectDetector.predict_img\u001b[0;34m(self, img, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_img\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 27\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miou\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_img \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:111\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:249\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m overrides\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m \u001b[43mTASK_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39msetup_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, verbose\u001b[38;5;241m=\u001b[39mis_cli)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/yolo/engine/predictor.py:108\u001b[0m, in \u001b[0;36mBasePredictor.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m _callbacks \u001b[38;5;129;01mor\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mget_default_callbacks()\n\u001b[0;32m--> 108\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_integration_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/yolo/utils/callbacks/base.py:199\u001b[0m, in \u001b[0;36madd_integration_callbacks\u001b[0;34m(instance)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.yolo.utils.callbacks.clearml'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class YOLOv8_ObjectDetector:\n",
    "\n",
    "    def __init__(self, model_file='yolov8n.pt', labels=None, classes=None, conf=0.25, iou=0.45):\n",
    "\n",
    "        self.classes = classes\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "        self.model = YOLO(model_file)\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "        if labels == None:\n",
    "            self.labels = self.model.names\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "        return results[0]\n",
    "\n",
    "    def default_display(self, show_conf=True, line_width=None, font_size=None,\n",
    "                        font='Arial.ttf', pil=False, example='abc'):\n",
    "        if self.results is None:\n",
    "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
    "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
    "        return display_img\n",
    "\n",
    "    def custom_display(self, colors, show_cls=True, show_conf=True):\n",
    "        img = self.orig_img\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "            x1, y1, x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            if show_cls:\n",
    "                textString += f\"{self.labels[class_id]}\"\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "            img = cv2.circle(img, center_coordinates, 5, (0, 0, 255), -1)\n",
    "\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0], y1 - textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString,\n",
    "                    (x1, y1), font,\n",
    "                    fontScale, (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def predict_and_save_images(self, image_dir, save_dir, display='custom', **display_args):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith(\".png\") or filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "                file_path = os.path.join(image_dir, filename)\n",
    "                img = cv2.imread(file_path)\n",
    "                results = self.predict_img(img, verbose=False)\n",
    "                if display == 'default':\n",
    "                    out_img = self.default_display(**display_args)\n",
    "                else:\n",
    "                    out_img = self.custom_display(**display_args)\n",
    "                cv2.imwrite(os.path.join(save_dir, filename), out_img)\n",
    "# Define the input and output directories\n",
    "image_dir = '/Users/martinprabhu/Downloads/YOLOV10/uavdt'\n",
    "save_dir = '/Users/martinprabhu/Downloads/YOLOV10/uavdt_detect'\n",
    "\n",
    "# Create an instance of the YOLOv8_ObjectDetector class\n",
    "detector = YOLOv8_ObjectDetector()\n",
    "\n",
    "# Define the colors for the bounding boxes\n",
    "colors = {0: (255, 0, 0),  # red for person\n",
    "          1: (0, 255, 0),  # green for car\n",
    "          2: (0, 0, 255)}  # blue for truck\n",
    "\n",
    "# Call the predict_and_save_images method to detect objects in all images in the input directory\n",
    "detector.predict_and_save_images(image_dir, save_dir, display='custom', colors=colors)\n",
    "\n",
    "print(f'Object detection completed. Results saved to {save_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (8.2.54)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from ultralytics) (1.24.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (3.5.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (4.7.0.72)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (9.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (1.9.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from ultralytics) (5.9.4)\n",
      "Requirement already satisfied: py-cpuinfo in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: six in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from cycler>=0.10->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/martinprabhu/Library/Python/3.11/lib/python/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/martinprabhu/Downloads/YOLOV10/uavdt_detect'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
